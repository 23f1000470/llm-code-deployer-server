import os, re, json, base64, time, asyncio
from typing import Dict, List, Optional
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx

app = FastAPI()

SECRET = os.getenv("SECRET", "")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")
GITHUB_USERNAME = os.getenv("GITHUB_USERNAME", "")
LLM_URL = (os.getenv("LLM_URL") or "").strip()
LLM_AUTH = (os.getenv("LLM_AUTH") or "").strip()
GITHUB_API = "https://api.github.com"

class Attachment(BaseModel):
    name: str
    url: str  # data URI

class TaskPayload(BaseModel):
    email: str
    secret: str
    task: str
    round: int
    nonce: str
    brief: str
    checks: List[str]
    evaluation_url: str
    attachments: List[Attachment] = []

MIT_LICENSE = """MIT License

Copyright (c) {year} {owner}

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software ... shortened ...
"""

PAGES_WORKFLOW = """name: GitHub Pages

on:
  push:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/configure-pages@v5
        with:
          enablement: true
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./
  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
"""

DATA_URI_RE = re.compile(r"^data:([\w/\-+.]+);base64,(.*)$", re.IGNORECASE)

async def gh(method: str, url: str, json_body=None, content=None, extra_headers=None):
    headers = {"Authorization": f"Bearer {GITHUB_TOKEN}", "Accept": "application/vnd.github+json"}
    if extra_headers: headers.update(extra_headers)
    async with httpx.AsyncClient(timeout=60) as client:
        return await client.request(method, url, json=json_body, headers=headers, content=content)

def parse_attachments(attachments: List[Attachment]) -> Dict[str, bytes]:
    out: Dict[str, bytes] = {}
    for a in attachments:
        m = DATA_URI_RE.match(a.url)
        if not m:
            raise ValueError(f"Attachment {a.name} is not a base64 data URI")
        out[a.name] = base64.b64decode(m.group(2))
    return out

async def ensure_pages_enabled(owner: str, repo: str):
    """One-time: enable Pages with build_type=workflow using PAT."""
    url = f"{GITHUB_API}/repos/{owner}/{repo}/pages"
    payload = {"build_type": "workflow"}
    r = await gh("PUT", url, json_body=payload)
    if r.status_code not in (201, 204):
        print("⚠️ Could not enable Pages automatically:", r.status_code, r.text)

async def ensure_repo(owner: str, repo: str) -> Dict:
    r = await gh("GET", f"{GITHUB_API}/repos/{owner}/{repo}")
    if r.status_code == 404:
        r = await gh("POST", f"{GITHUB_API}/user/repos", json_body={
            "name": repo,
            "private": False,
            "auto_init": True,
            "description": "Auto-generated by LLM code deployer"
        })
        r.raise_for_status()
    else:
        r.raise_for_status()
    repo_json = r.json()
    # ensure Pages is enabled
    await ensure_pages_enabled(owner, repo)
    return repo_json

async def put_file(owner: str, repo: str, path: str, content_bytes: bytes, message: str):
    sha = None
    r = await gh("GET", f"{GITHUB_API}/repos/{owner}/{repo}/contents/{path}")
    if r.status_code == 200:
        sha = r.json().get("sha")
    enc = base64.b64encode(content_bytes).decode()
    payload = {"message": message, "content": enc}
    if sha:
        payload["sha"] = sha
    r = await gh("PUT", f"{GITHUB_API}/repos/{owner}/{repo}/contents/{path}", json_body=payload)
    r.raise_for_status()
    return r.json()["commit"]["sha"]

async def wait_for_200(url: str, timeout_s: int = 180):
    start = time.time()
    async with httpx.AsyncClient(timeout=15) as client:
        while time.time() - start < timeout_s:
            try:
                r = await client.get(url, headers={"Cache-Control": "no-cache"})
                if r.status_code == 200:
                    return True
            except Exception:
                pass
            await asyncio.sleep(3)
    return False

async def post_with_backoff(url: str, payload: dict, max_tries: int = 8):
    delay = 1
    async with httpx.AsyncClient(timeout=30) as client:
        for _ in range(max_tries):
            try:
                r = await client.post(url, json=payload, headers={"Content-Type": "application/json"})
                if r.status_code == 200:
                    return True
            except Exception:
                pass
            await asyncio.sleep(delay)
            delay *= 2
    return False

def template_from_brief(brief: str, atts: Dict[str, bytes]) -> Dict[str, bytes]:
    b = brief.lower()
    files: Dict[str, bytes] = {}
    if "sales" in b and "csv" in b:
        html = """<!doctype html><html><head>
<meta charset="utf-8"><title>Sales Summary</title>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
</head><body class="container p-4">
<h1>Sales Summary</h1>
<p>Total: <strong id="total-sales">0</strong></p>
<script>
(async () => {
  const text = await fetch('data.csv').then(r=>r.text());
  const rows = text.trim().split(/\\n+/).slice(1).map(l=>l.split(','));
  let sum=0;
  for (const [p,v] of rows) {
    const n=parseFloat(v);
    if(!isNaN(n)) sum+=n;
  }
  document.querySelector('#total-sales').textContent = sum.toFixed(2);
})();
</script>
</body></html>"""
        files["index.html"] = html.encode()
        if "data.csv" not in atts:
            files["data.csv"] = b"product,sales\nA,10\nB,20.5\n"
        return files
    files["index.html"] = f"<html><body><h1>{brief}</h1></body></html>".encode()
    return files

async def commit_bundle(owner: str, repo: str, files: Dict[str, bytes], brief: str, task: str, round_i: int, pages_url: str) -> str:
    commit_sha = await put_file(owner, repo, "LICENSE", MIT_LICENSE.format(year=time.strftime("%Y"), owner=owner).encode(), f"[{task}] LICENSE")
    readme = f"# {repo}\\n\\nTask: {task} round {round_i}\\nBrief: {brief}\\nPages: {pages_url}\\n"
    commit_sha = await put_file(owner, repo, "README.md", readme.encode(), f"[{task}] README")
    commit_sha = await put_file(owner, repo, ".github/workflows/pages.yml", PAGES_WORKFLOW.encode(), f"[{task}] workflow")
    for path, blob in files.items():
        commit_sha = await put_file(owner, repo, path, blob, f"[{task}] {path}")
    return commit_sha

@app.get("/")
def root():
    return {"status": "ok"}

@app.post("/api-task")
async def api_task(body: TaskPayload):
    if body.secret != SECRET:
        raise HTTPException(status_code=403, detail="invalid secret")
    att_bytes = parse_attachments(body.attachments)
    files = template_from_brief(body.brief, att_bytes)
    for k,v in att_bytes.items():
        files.setdefault(k, v)
    repo_name = body.task.replace("/", "-")
    pages_url = f"https://{GITHUB_USERNAME}.github.io/{repo_name}/"
    await ensure_repo(GITHUB_USERNAME, repo_name)
    commit_sha = await commit_bundle(GITHUB_USERNAME, repo_name, files, body.brief, body.task, body.round, pages_url)
    await wait_for_200(pages_url, 180)
    resp = {
        "email": body.email,
        "task": body.task,
        "round": body.round,
        "nonce": body.nonce,
        "repo_url": f"https://github.com/{GITHUB_USERNAME}/{repo_name}",
        "commit_sha": commit_sha,
        "pages_url": pages_url,
    }
    await post_with_backoff(body.evaluation_url, resp)
    return resp
