import os, re, json, base64, time, asyncio
from typing import Dict, List, Optional, Any
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx

app = FastAPI()

# ===== ENV =====
SECRET = os.getenv("SECRET", "")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")
GITHUB_USERNAME = os.getenv("GITHUB_USERNAME", "")
LLM_URL = (os.getenv("LLM_URL") or "").strip()           # e.g. https://aipipe.org/openrouter/v1
LLM_AUTH = (os.getenv("LLM_AUTH") or "").strip()         # e.g. Bearer AIPIPE_TOKEN
GITHUB_API = "https://api.github.com"

# ===== MODELS =====
class Attachment(BaseModel):
    name: str
    url: str  # base64 data URI

class TaskPayload(BaseModel):
    email: str
    secret: str
    task: str
    round: int
    nonce: str
    brief: str
    checks: List[str]
    evaluation_url: str
    attachments: List[Attachment] = []

# ===== CONSTANTS =====
MIT_LICENSE = """MIT License

Copyright (c) {year} {owner}

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""

# Minimal single-job Pages workflow
PAGES_WORKFLOW = """name: GitHub Pages

on:
  push:
    branches: [ main ]
    paths:
      - "index.html"
      - "script.js"
      - "styles.css"
      - "assets/**"
      - "data.*"
      - ".github/workflows/pages.yml"
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Upload static site
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./
  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
"""

# Safe regex for data URIs (dash escaped)
DATA_URI_RE = re.compile(r"^data:([\w/.\+\-]+);base64,(.*)$", re.IGNORECASE)

# ===== HTTP helper =====
async def gh(method: str, url: str, json_body=None, content=None, extra_headers=None) -> httpx.Response:
    headers = {"Authorization": f"Bearer {GITHUB_TOKEN}", "Accept": "application/vnd.github+json"}
    if extra_headers:
        headers.update(extra_headers)
    async with httpx.AsyncClient(timeout=60) as client:
        return await client.request(method, url, json=json_body, headers=headers, content=content)

def b64e(b: bytes) -> str:
    return base64.b64encode(b).decode()

def parse_attachments(attachments: List[Attachment]) -> Dict[str, bytes]:
    out: Dict[str, bytes] = {}
    for a in attachments:
        m = DATA_URI_RE.match(a.url)
        if not m:
            raise ValueError(f"Attachment {a.name} is not a base64 data URI")
        out[a.name] = base64.b64decode(m.group(2))
    return out

def decode_possible_data_uri_or_text(val: str) -> bytes:
    m = DATA_URI_RE.match(val.strip())
    if m:
        return base64.b64decode(m.group(2))
    return val.encode("utf-8")

# ===== GitHub automation =====
async def ensure_repo(owner: str, repo: str) -> Dict[str, Any]:
    r = await gh("GET", f"{GITHUB_API}/repos/{owner}/{repo}")
    if r.status_code == 404:
        r = await gh("POST", f"{GITHUB_API}/user/repos", json_body={
            "name": repo, "private": False, "auto_init": True,
            "description": "Auto-generated by LLM code deployer"
        })
        r.raise_for_status()
    else:
        r.raise_for_status()
    return r.json()

async def ensure_actions_write_permissions(owner: str, repo: str):
    url = f"{GITHUB_API}/repos/{owner}/{repo}/actions/permissions/workflow"
    payload = {"default_workflow_permissions": "write", "can_approve_pull_request_reviews": False}
    r = await gh("PUT", url, json_body=payload)
    if r.status_code not in (200, 204):
        print("⚠️ Could not set workflow write perms:", r.status_code, r.text)

async def ensure_pages_site(owner: str, repo: str):
    get_url = f"{GITHUB_API}/repos/{owner}/{repo}/pages"
    r = await gh("GET", get_url)
    if r.status_code == 404:
        r2 = await gh("POST", get_url, json_body={"build_type": "workflow"})
        if r2.status_code not in (201, 204):
            print("⚠️ Pages create failed:", r2.status_code, r2.text)
    elif r.status_code == 200:
        data = r.json()
        if data.get("build_type") != "workflow":
            r2 = await gh("PUT", get_url, json_body={"build_type": "workflow"})
            if r2.status_code not in (200, 204):
                print("⚠️ Pages update failed:", r2.status_code, r2.text)
    else:
        print("⚠️ Pages GET unexpected:", r.status_code, r.text)

async def batch_commit(owner: str, repo: str, files: Dict[str, bytes], message: str) -> str:
    r = await gh("GET", f"{GITHUB_API}/repos/{owner}/{repo}/git/ref/heads/main")
    r.raise_for_status()
    base_commit = r.json()["object"]["sha"]

    r = await gh("GET", f"{GITHUB_API}/repos/{owner}/{repo}/git/commits/{base_commit}")
    r.raise_for_status()
    base_tree = r.json()["tree"]["sha"]

    entries = []
    for path, blob in files.items():
        rb = await gh("POST", f"{GITHUB_API}/repos/{owner}/{repo}/git/blobs",
                      json_body={"content": b64e(blob), "encoding": "base64"})
        rb.raise_for_status()
        entries.append({"path": path, "mode": "100644", "type": "blob", "sha": rb.json()["sha"]})

    rt = await gh("POST", f"{GITHUB_API}/repos/{owner}/{repo}/git/trees",
                  json_body={"base_tree": base_tree, "tree": entries})
    rt.raise_for_status()
    new_tree = rt.json()["sha"]

    rc = await gh("POST", f"{GITHUB_API}/repos/{owner}/{repo}/git/commits",
                  json_body={"message": message, "tree": new_tree, "parents": [base_commit]})
    rc.raise_for_status()
    new_commit = rc.json()["sha"]

    rr = await gh("PATCH", f"{GITHUB_API}/repos/{owner}/{repo}/git/refs/heads/main",
                  json_body={"sha": new_commit, "force": True})
    rr.raise_for_status()
    return new_commit

async def wait_for_200(url: str, timeout_s: int = 180) -> bool:
    start = time.time()
    async with httpx.AsyncClient(timeout=15) as client:
        last = None
        while time.time() - start < timeout_s:
            try:
                r = await client.get(url, headers={"Cache-Control": "no-cache"})
                last = r.status_code
                if r.status_code == 200:
                    return True
            except Exception:
                pass
            await asyncio.sleep(3)
    print(f"⚠️ Pages not 200 within {timeout_s}s (last={last})")
    return False

async def post_with_backoff(url: str, payload: dict, max_tries: int = 8) -> bool:
    delay = 1
    async with httpx.AsyncClient(timeout=30) as client:
        for _ in range(max_tries):
            try:
                r = await client.post(url, json=payload, headers={"Content-Type": "application/json"})
                if r.status_code == 200:
                    return True
            except Exception:
                pass
            await asyncio.sleep(delay)
            delay *= 2
    return False

# ===== LLM calls =====
def _llm_endpoint(path: str) -> str:
    return LLM_URL.rstrip("/") + path

async def llm_chat_json(messages: List[Dict[str, str]], model: str = "google/gemini-2.0-flash-lite-001") -> Optional[dict]:
    if not (LLM_URL and LLM_AUTH):
        return None
    headers = {"Authorization": LLM_AUTH, "Content-Type": "application/json"}
    body = {
        "model": model,
        "messages": messages,
        "temperature": 0.2,
        "response_format": {"type": "json_object"},
    }
    url = _llm_endpoint("/chat/completions")
    try:
        async with httpx.AsyncClient(timeout=90) as client:
            r = await client.post(url, headers=headers, json=body)
            r.raise_for_status()
            data = r.json()
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "{}")
            return json.loads(content)
    except Exception as e:
        print("LLM error:", repr(e))
        return None

# ---- Option B: planner then builder ----
PLANNER_SYSTEM = "Return STRICT JSON only, as {\"spec\":{...}}. No prose."
PLANNER_USER_TMPL = """Expand this brief into a build SPEC for a static GitHub Pages app.

Include keys:
- files: list of file paths to create (must include: index.html, script.js, styles.css, README.md; plus any assets like assets/sample.png or data.csv)
- dom_ids: ids required by the brief/tests (e.g., ["total-sales","github-created-at","captcha-text","stats-products","stats-total","sales-pie","theme-toggle"])
- data_inputs: list of inputs you need (attachments, ?url params, remote APIs). For remote HTTP from browser, use proxy `https://aipipe.org/proxy/<ENCODED_URL>` to avoid CORS.
- algorithms: stepwise logic to compute outputs from inputs
- assets_needed: filenames and brief content/format for defaults (e.g., data.csv rows, a sample captcha image, rates.json)
- fallbacks: what to do if ?url or inputs are missing (must gracefully render)
- round2_extensions: suggested future changes (tables, filters, aria-live, currency, caching, etc.)

BRIEF:
{brief}

ATTACHMENTS:
{attachment_names}
"""

BUILDER_SYSTEM = "Return STRICT JSON only, as {\"files\":{...}}. No prose."
BUILDER_USER_TMPL = """Using this SPEC, generate a browser-only static site suitable for GitHub Pages.

Rules:
1) Create these files at minimum: index.html, script.js, styles.css, README.md. Additional assets under /assets or in root (e.g., data.csv).
2) index.html
   - Load Bootstrap 5 via jsDelivr.
   - Link ./styles.css and ./script.js (relative).
   - Provide accessible markup; aria-live regions for status when relevant.
3) script.js
   - Parse URL params (?url, ?token, etc).
   - For cross-origin fetches, wrap with the proxy:
       const proxied = (u) => u.startsWith('http') ? `https://aipipe.org/proxy/${{encodeURIComponent(u)}}` : u;
       await fetch(proxied(url))
   - If SPEC says an input may be missing, use provided default assets (e.g., ./assets/sample.png or ./data.csv).
   - Render outputs into the DOM IDs promised in SPEC (e.g., #total-sales).
   - Be idempotent: re-rendering should not duplicate table rows; totals stay accurate after any updates.
4) styles.css: basic layout and spacing; demonstrate clamp() and calc().
5) README.md: include Task, Brief, How to run locally, How it works (inputs, IDs), License.
6) Keep code minimal but correct.

SPEC:
{spec_json}
"""

# ---- Option A: single-shot builder ----
SINGLE_BUILDER_SYSTEM = "Return STRICT JSON only, as {\"files\":{...}}. No prose."
SINGLE_BUILDER_USER_TMPL = """Build a browser-only static site for GitHub Pages that satisfies the brief.

Requirements:
- MUST return: index.html, script.js, styles.css, README.md; plus any needed assets (e.g., assets/sample.png, data.csv).
- index.html links ./styles.css and ./script.js; loads Bootstrap 5.
- script.js handles ?url params; uses AIpipe CORS proxy for remote fetches; graceful fallbacks (local assets) if inputs missing.
- Render outputs using the exact IDs mentioned in the brief.
- README explains brief, setup, usage, and license.

BRIEF:
{brief}

ATTACHMENTS:
{attachment_names}
"""

# ===== Deterministic templates =====
def sales_round1_files() -> Dict[str, bytes]:
    index_html = """<!doctype html><html><head>
<meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1">
<title>Sales Summary</title>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
<link rel="stylesheet" href="./styles.css">
</head><body class="container py-4">
<h1>Sales Summary</h1>
<p>Total: <strong id="total-sales">0</strong></p>
<table id="product-sales" class="table table-striped d-none">
  <thead><tr><th>Product</th><th>Sales</th></tr></thead><tbody></tbody>
</table>
<script src="./script.js"></script>
</body></html>"""
    script_js = r"""(async () => {
  const proxied = (u) => u.startsWith('http') ? `https://aipipe.org/proxy/${encodeURIComponent(u)}` : u;
  const url = new URLSearchParams(location.search).get('url') || 'data.csv';
  const text = await fetch(proxied(url)).then(r=>r.text()).catch(()=> '');
  const rows = text.trim() ? text.trim().split(/\n+/).slice(1).map(l=>l.split(',')) : [];
  let sum=0; const by={};
  for (const [p,v] of rows) { const n=parseFloat(v); if(!isNaN(n)){ sum+=n; by[p]=(by[p]||0)+n; } }
  document.querySelector('#total-sales').textContent = (sum||0).toFixed(2);
  const tbody = document.querySelector('#product-sales tbody'); tbody.innerHTML='';
  for (const [k,v] of Object.entries(by)) {
    const tr = document.createElement('tr');
    tr.innerHTML = `<td>${k}</td><td>${v.toFixed(2)}</td>`;
    tbody.appendChild(tr);
  }
  if (Object.keys(by).length) document.querySelector('#product-sales').classList.remove('d-none');
})();"""
    styles_css = "h1{font-size:clamp(1.5rem,2.5vw,2.25rem)} body{line-height:1.6}"
    data_csv = "product,sales\nA,10\nB,20.5\n"
    return {
        "index.html": index_html.encode(),
        "script.js": script_js.encode(),
        "styles.css": styles_css.encode(),
        "data.csv": data_csv.encode(),
    }

def sales_round2_files() -> Dict[str, bytes]:
    index_html = """<!doctype html><html><head>
<meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1">
<title>Sales Summary</title>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
<link rel="stylesheet" href="./styles.css">
</head>
<body class="container py-4">
  <div class="d-flex align-items-center justify-content-between mb-3">
    <h1 class="m-0">Sales Summary</h1>
    <button id="theme-toggle" class="btn btn-outline-secondary">Toggle Theme</button>
  </div>

  <table id="stats-table" class="table table-bordered w-auto mb-4">
    <thead><tr><th>Metric</th><th>Value</th></tr></thead>
    <tbody>
      <tr><td>Total Products</td><td id="stats-products">0</td></tr>
      <tr><td>Total Sales</td><td id="stats-total">0</td></tr>
    </tbody>
  </table>

  <p>Overall Total: <strong id="total-sales">0</strong></p>

  <div class="row">
    <div class="col-12 col-lg-6">
      <table id="product-sales" class="table table-striped">
        <thead><tr><th>Product</th><th>Sales</th></tr></thead><tbody></tbody>
      </table>
    </div>
    <div class="col-12 col-lg-6">
      <canvas id="sales-pie" height="240"></canvas>
    </div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="./script.js"></script>
</body></html>"""
    script_js = r"""(() => {
  const $ = (sel) => document.querySelector(sel);
  const proxied = (u) => u && u.startsWith('http') ? `https://aipipe.org/proxy/${encodeURIComponent(u)}` : u;

  let chart;

  async function loadData() {
    const url = new URLSearchParams(location.search).get('url') || 'data.csv';
    const text = await fetch(proxied(url)).then(r=>r.text()).catch(()=> '');
    const rows = text.trim() ? text.trim().split(/\n+/).slice(1).map(l=>l.split(',')) : [];
    const by = {}, products = new Set();
    let total = 0;
    for (const [p, v] of rows) {
      const n = parseFloat(v);
      if (!isNaN(n)) { total += n; by[p] = (by[p]||0)+n; products.add(p); }
    }
    // Stats
    $('#total-sales').textContent = total.toFixed(2);
    $('#stats-total').textContent = total.toFixed(2);
    $('#stats-products').textContent = String(products.size);

    // Table
    const tbody = $('#product-sales tbody');
    tbody.innerHTML = '';
    for (const [k, v] of Object.entries(by)) {
      const tr = document.createElement('tr');
      tr.innerHTML = `<td>${k}</td><td>${v.toFixed(2)}</td>`;
      tbody.appendChild(tr);
    }

    // Chart
    const labels = Object.keys(by);
    const data = Object.values(by);
    const ctx = document.getElementById('sales-pie').getContext('2d');
    const isDark = document.documentElement.dataset.theme === 'dark';
    const border = isDark ? '#ddd' : '#333';

    if (chart) chart.destroy();
    chart = new Chart(ctx, {
      type: 'pie',
      data: { labels, datasets: [{ data }] },
      options: {
        plugins: { legend: { labels: { color: border } } }
      }
    });
  }

  function toggleTheme() {
    const root = document.documentElement;
    const next = root.dataset.theme === 'dark' ? 'light' : 'dark';
    root.dataset.theme = next;
    // Re-draw chart to match colors
    loadData();
  }

  $('#theme-toggle').addEventListener('click', toggleTheme);
  // initial
  loadData();
})();"""
    styles_css = r""":root{
  --bg: #ffffff;
  --fg: #111111;
  --muted: #6c757d;
  --space: clamp(0.5rem, 1.5vw, 1rem);
}
:root[data-theme="dark"]{
  --bg: #0f1115;
  --fg: #e7e7e7;
  --muted: #9aa0a6;
}
html,body{background:var(--bg);color:var(--fg);}
h1{font-size:clamp(1.6rem, 3vw, 2.4rem); margin-bottom: calc(var(--space) * 1.5);}
.table{margin-bottom: calc(var(--space) * 2);}
.btn{margin-left: var(--space);}
"""
    return {
        "index.html": index_html.encode(),
        "script.js": script_js.encode(),
        "styles.css": styles_css.encode(),
    }

def generic_files(brief: str) -> Dict[str, bytes]:
    return {
        "index.html": f"<!doctype html><html><head><meta charset='utf-8'><title>Generated App</title><link rel='stylesheet' href='./styles.css'></head><body class='container py-4'><h1>Generated App</h1><pre id='brief'></pre><script src='./script.js'></script></body></html>".encode(),
        "script.js": f"document.querySelector('#brief').textContent = {json.dumps(brief)};".encode(),
        "styles.css": b"body{font-family:system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif}",
    }

def choose_template(brief: str, round_i: int) -> Dict[str, bytes]:
    b = brief.lower()
    if "sum-of-sales" in b or ("sales" in b and "csv" in b):
        return sales_round2_files() if round_i >= 2 else sales_round1_files()
    return generic_files(brief)

# ===== Validation =====
def valid_files(files: Dict[str, bytes]) -> bool:
    req = {"index.html", "script.js", "styles.css"}
    # README.md will be added if missing
    return req.issubset(set(files.keys()))

# ===== ROUTES =====
@app.get("/")
def root():
    return {"status": "ok", "mode": "planner->builder with Round-2 deterministic fallback"}

@app.post("/api-task")
async def api_task(body: TaskPayload):
    if body.secret != SECRET:
        raise HTTPException(status_code=403, detail="invalid secret")
    if not GITHUB_TOKEN or not GITHUB_USERNAME:
        raise HTTPException(status_code=500, detail="server missing GitHub config")

    # attachments
    att_bytes = parse_attachments(body.attachments)
    att_names = list(att_bytes.keys())

    # try planner->builder, then single-shot
    files_text: Optional[Dict[str, str]] = None
    if LLM_URL and LLM_AUTH:
        # planner
        planner = await llm_chat_json(
            [{"role": "system", "content": PLANNER_SYSTEM},
             {"role": "user", "content": PLANNER_USER_TMPL.format(brief=body.brief, attachment_names=att_names)}]
        )
        spec = planner.get("spec") if isinstance(planner, dict) else None
        # builder
        if isinstance(spec, dict):
            built = await llm_chat_json(
                [{"role": "system", "content": BUILDER_SYSTEM},
                 {"role": "user", "content": BUILDER_USER_TMPL.format(spec_json=json.dumps(spec))}]
            )
            if isinstance(built, dict) and isinstance(built.get("files"), dict):
                files_text = {k: str(v) for k, v in built["files"].items()}
        # single-shot
        if files_text is None:
            single = await llm_chat_json(
                [{"role": "system", "content": SINGLE_BUILDER_SYSTEM},
                 {"role": "user", "content": SINGLE_BUILDER_USER_TMPL.format(brief=body.brief, attachment_names=att_names)}]
            )
            if isinstance(single, dict) and isinstance(single.get("files"), dict):
                files_text = {k: str(v) for k, v in single["files"].items()}

    # convert to bytes
    files: Dict[str, bytes] = {}
    if files_text:
        for path, val in files_text.items():
            files[path] = decode_possible_data_uri_or_text(val)

    # fallback (now round-aware)
    if not files or not valid_files(files):
        files = choose_template(body.brief, body.round)

    # force-overwrite attachments
    for name, blob in att_bytes.items():
        files[name] = blob

    repo_name = body.task.replace("/", "-")
    pages_url = f"https://{GITHUB_USERNAME}.github.io/{repo_name}/"
    await ensure_repo(GITHUB_USERNAME, repo_name)
    await ensure_actions_write_permissions(GITHUB_USERNAME, repo_name)
    await ensure_pages_site(GITHUB_USERNAME, repo_name)

    # add license/readme/workflow in one commit
    all_files: Dict[str, bytes] = {}
    all_files["LICENSE"] = MIT_LICENSE.format(year=time.strftime("%Y"), owner=GITHUB_USERNAME).encode()
    # keep README from LLM if present, else synthesize
    readme_auto = f"# {repo_name}\n\nTask: `{body.task}` (round {body.round})\n\nBrief:\n\n{body.brief}\n\nPages: {pages_url}\n\nLicense: MIT\n"
    all_files["README.md"] = files.get("README.md", readme_auto.encode())
    all_files[".github/workflows/pages.yml"] = PAGES_WORKFLOW.encode()
    for p, b in files.items():
        if p == "README.md":
            continue
        all_files[p] = b

    commit_sha = await batch_commit(GITHUB_USERNAME, repo_name, all_files, f"[{body.task}] round {body.round} deploy")

    await wait_for_200(pages_url, timeout_s=180)

    resp = {
        "email": body.email,
        "task": body.task,
        "round": body.round,
        "nonce": body.nonce,
        "repo_url": f"https://github.com/{GITHUB_USERNAME}/{repo_name}",
        "commit_sha": commit_sha,
        "pages_url": pages_url,
    }
    await post_with_backoff(body.evaluation_url, resp)
    return resp
